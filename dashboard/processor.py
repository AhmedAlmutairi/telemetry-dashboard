try:
    import simplejson as json
except ImportError:
    import json
from helpers import parse_input
from datetime import datetime
import math, sys, os
# Import histogram specs and generated by makefile using specgen.py
# This is imported the zipfile that this module was loaded from
HistogramAggregator = __loader__.load_module("auxiliary").HistogramAggregator


# Counts number of times we've printed a log message
logMsgCount = {}

# Auxiliary method to write log messages
def log(msg, *args):
    global logMsgCount
    # We only print a log message the first 10 times we see it
    n = logMsgCount.get(msg, 10)
    if n > 0:
        logMsgCount[msg] = n - 1
        print >> sys.stderr, msg % args


############################## Ugly hacks for simple measures

# Auxiliary method for computing bucket offsets from parameters, it is stolen
# from histogram_tools.py, though slightly modified...
def exponential_buckets(dmin, dmax, n_buckets):
    log_max = math.log(dmax);
    ret_array = [0] * n_buckets
    current = dmin
    ret_array[1] = current
    for bucket_index in range(2, n_buckets):
        log_current = math.log(current)
        log_ratio = (log_max - log_current) / (n_buckets - bucket_index)
        log_next = log_current + log_ratio
        next_value = int(math.floor(math.exp(log_next) + 0.5))
        if next_value > current:
            current = next_value
        else:
            current = current + 1
        ret_array[bucket_index] = current
    return ret_array

# Create buckets from buckets2index from ranges... snippet pretty much stolen
# from specgen.py
def buckets2index_from_ranges(ranges):
    buckets = map(str, ranges)
    bucket2index = {}
    for i in range(0, len(buckets)):
        bucket2index[buckets[i]] = i
    return bucket2index

# Bucket offsets for simple measures
simple_measures_buckets =   (
                                buckets2index_from_ranges(
                                        exponential_buckets(1, 30000, 50)
                                    ),
                                    exponential_buckets(1, 30000, 50)
                            )

############################## End of ugly hacks for simple measures

class Processor:
    def __init__(self, output_folder):
        self.output_folder = output_folder
        self.cache = {}

    @parse_input
    def process(self, uid, dimensions, payload):
        # Unpack dimensions
        reason, appName, channel, version, buildId, submissionDate = dimensions

        # Get OS, osVersion and architecture information
        info = payload['info']
        OS = info['OS']
        osVersion = str(info['version'])
        arch = info['arch']
        revision = info['revision']

        # Get the major version
        majorVersion = version.split('.')[0]

        if OS == "Linux":
            osVersion = osVersion[:3]

        # Get the build date, ignore the rest of the buildId
        buildDate = buildId[:8]


        filterPath = (buildDate, reason, appName, OS, osVersion, arch)
        self.create_aggregates(filterPath, channel, majorVersion, "by-build-date",
                               payload, buildId, revision)

        # Aggregate histograms by submission date, if time since build is less
        # than 60 days
        bdate = datetime.strptime(buildDate, "%Y%m%d")
        sdate = datetime.strptime(submissionDate, "%Y%m%d")
        if (sdate - bdate).days < 60:
            filterPath = (buildDate, reason, appName, OS, osVersion, arch)

            self.create_aggregates(filterPath, channel, majorVersion,
                                   "by-submission-date", payload, buildId,
                                   revision)

    def create_aggregates(self, filterPath, channel, majorVersion, byDateType, payload, buildId, revision):
        # Aggregate histograms
        for name, values in payload.get('histograms', {}).iteritems():
            filePath = (channel, majorVersion, name, byDateType)

            # Find cache set for filePath
            cacheSet = self.cache.setdefault(filePath, {})
            # Find aggregator for filter path
            aggregator = cacheSet.get(filterPath, None)
            if aggregator == None:
                aggregator = HistogramAggregator()
                cacheSet[filterPath] = aggregator

            # Aggregate values
            aggregator.merge(values + [1], buildId, revision)

        # Aggregate simple measurements
        for name, value in payload.get('simpleMeasurements', {}).iteritems():
            # Handle cases where the value is a dictionary of simple measures
            if type(value) == dict:
                for subName, subValue in value.iteritems():
                    self.aggregate_simple_measure(channel, majorVersion, filterPath,
                                                  name + "_" + str(subName),
                                                  byDateType, subValue)
            else:
                self.aggregate_simple_measure(channel, majorVersion, filterPath,
                                              name, byDateType, value)

    def aggregate_simple_measure(self, channel, majorVersion, filterPath, name,
                                 byDateType, value):
        # Sanity check value
        if type(value) not in (int, long, float):
            log("%s is not a value type for simpleMeasurements \"%s\"",
                type(value), name)
            return

        bucket = simple_measures_buckets[1]
        values = [0] * (len(bucket) + 6)
        for i in reversed(range(0, len(bucket))):
            if value >= bucket[i]:
                values[i] = 1
                break

        log_val = math.log(math.fabs(value) + 1)
        values[-6] = value                # sum
        values[-5] = log_val              # log_sum
        values[-4] = log_val * log_val    # log_sum_squares
        values[-3] = 0                    # sum_squares_lo
        values[-2] = 0                    # sum_squares_hi
        values[-1] = 1                    # count


        filePath = (channel, majorVersion, "SIMPLE_MEASURES_" + name.upper(), byDateType)

        # Find cache set for filePath
        cacheSet = self.cache.setdefault(filePath, {})
        # Find aggregator for filter path
        aggregator = cacheSet.get(filterPath, None)
        if aggregator == None:
            aggregator = HistogramAggregator()
            cacheSet[filterPath] = aggregator

        # Aggregate values
        aggregator.merge(values, "0", "simple-measures-hack")

    def flush(self):
        with open(os.path.join(self.output_folder, "result.txt"), "w") as out:
            for filePath, cacheSet in self.cache.iteritems():
                output = {}
                for filterPath, aggregator in cacheSet.iteritems():
                    output["/".join(filterPath)] = aggregator.dump()
                out.write("/".join(filePath) + "\t" + json.dumps(output) + "\n")
